{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de  cnn_text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT9aHFptEZdb"
      },
      "source": [
        "<center><h2>DSSP<br>Deep Learning - Convolution Neural Networks</h2><h3>1D Convolutional Neural Networks for Text Categorization</h3> 8 / 12/ 2020<br> K. Skianis, H. Abdine</center>\n",
        "\n",
        "This part of the lab is largely based on Antoine Tixier's notes [Introduction to CNNs and LSTMs for NLP](https://arxiv.org/pdf/1808.09772.pdf). You are strongly encouraged to have a look at these notes for a quick theoretical intro.\n",
        "\n",
        "In this part of the lab, we will implement a convolutional neural network (CNN) to perform binary movie review classification (positive/negative) using the [Keras](https://keras.io) library. The architecture of the CNN we will develop is described in [CNNs for Sentence Classification (Kim, EMNLP'14)](https://www.aclweb.org/anthology/D14-1181). We will also visualize document embeddings and predictive regions in the input documents, following [Effective Use of Word Order for Text Categorization with Convolutional Neural Networks (Johnson and Zhang, NAACL'15)](https://www.aclweb.org/anthology/N15-1011/), and first-order derivate saliency maps, following [Visualizing and Understanding Neural Models in NLP (Li et al., NAACL'16)](http://www.aclweb.org/anthology/N16-1082). \n",
        "\n",
        "\n",
        "### Reading data and preprocessing\n",
        "For our experiments, we will use the [sentence polarity dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz). The dataset was collected by Pang and Lee and consists of 5,331 positive and 5,331 negative snippets acquired from Rotten Tomatoes. Snippets were automatically labeled using the labels provided by Rotten Tomatoes. The positive and negative reviews are stored into the `rt-polarity.pos` and `rt-polarity.neg` files, respectively. Let's first read the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRpRaFn2EsNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e75bf70-288b-491b-f77d-6953c01998c0"
      },
      "source": [
        "import urllib\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Initialize lists for data and class labels\n",
        "docs = list()\n",
        "class_labels = list()\n",
        "\n",
        "url = 'https://resourcesftp.blob.core.windows.net/files/dssp/deep_learning/data/rt-polarity.pos'\n",
        "file = urllib.request.urlopen(url)\n",
        "\n",
        "# For each row of the csv file\n",
        "for line in file:\n",
        "  docs.append(line.decode('utf-8', 'ignore'))\n",
        "  class_labels.append(1)\n",
        "\n",
        "\n",
        "url = 'https://resourcesftp.blob.core.windows.net/files/dssp/deep_learning/data/rt-polarity.neg'\n",
        "file = urllib.request.urlopen(url)\n",
        "\n",
        "# For each row of the csv file\n",
        "for line in file:\n",
        "  docs.append(line.decode('utf-8', 'ignore'))\n",
        "  class_labels.append(0)\n",
        "\n",
        "y = to_categorical(class_labels)\n",
        "\n",
        "# your code here\n",
        "print(\"first positive r\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itw5Y5KpGeqW"
      },
      "source": [
        "The documents that are contained in the dataset have already undergone some preprocessing. Therefore, we will only remove some punctuation marks, diacritics, and non letters, if any. Furthermore, we will represent each document as a list of tokens. Use the ``preprocessing`` function (already implemented) to preprocess the documents.\n",
        "\n",
        "Task:\n",
        "- print a positive and negative document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5WtnNxuGfWd"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_str(string):\n",
        "  string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
        "  string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
        "  string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
        "  string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
        "  string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
        "  string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
        "  string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "  string = re.sub(r\",\", \" , \", string) \n",
        "  string = re.sub(r\"!\", \" ! \", string) \n",
        "  string = re.sub(r\"\\(\", \" \\( \", string) \n",
        "  string = re.sub(r\"\\)\", \" \\) \", string) \n",
        "  string = re.sub(r\"\\?\", \" \\? \", string) \n",
        "  string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "  return string.strip().split()\n",
        "\n",
        "    \n",
        "def preprocessing(docs):\n",
        "  preprocessed_docs = []\n",
        "\n",
        "  for doc in docs:\n",
        "    preprocessed_docs.append(clean_str(doc))\n",
        "\n",
        "  return preprocessed_docs\n",
        "\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GR-qxVbGhsi"
      },
      "source": [
        "Subsequently, we will extract the vocabulary of the dataset. We will store the vocabulary in a dictionary where keys are terms and values correspond to indices. Hence, each term will be assigned a unique index. The minimum index will be equal to 1, while the maximum index will be equal to the size of the vocabulary.\n",
        "\n",
        "Task:\n",
        "- get the vocabulary and print it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U2xJRgkGiQ9"
      },
      "source": [
        "def get_vocab(processed_docs):\n",
        "  vocab = dict()\n",
        "\n",
        "  for doc in processed_docs:\n",
        "    for word in doc:\n",
        "      if word not in vocab:\n",
        "        vocab[word] = len(vocab) + 1\n",
        "\n",
        "  return vocab\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzD1jBjlGkOj"
      },
      "source": [
        "Next, we will load a set of 300-dimensional word embeddings learned with word2vec on the GoogleNews dataset. The embeddings can be downloaded from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit. Using `gensim`, we can extract only the vectors of the words found in our vocabulary. Terms not present in the set of pre-trained words are initialized randomly (uniformly in [−0.25, 0.25]). Before executing the code, set the path for the file that contains the word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iOHkYS8GlW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1434cd2a-bb6b-43ab-b950-475363896107"
      },
      "source": [
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-07 20:09:08--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.38.214\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.38.214|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  30.6MB/s    in 22s     \n",
            "\n",
            "2020-12-07 20:09:31 (70.1 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5PJpTjsGmds"
      },
      "source": [
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "def load_embeddings(fname, vocab):\n",
        "  embeddings = np.zeros((len(vocab)+1, 300))\n",
        "  \n",
        "  model = KeyedVectors.load_word2vec_format(fname, binary=True)\n",
        "  for word in vocab:\n",
        "    if word in model:\n",
        "      embeddings[vocab[word]] = model[word]\n",
        "    else:\n",
        "      embeddings[vocab[word]] = np.random.uniform(-0.25, 0.25, 300)\n",
        "\n",
        "  return embeddings\n",
        "\n",
        "\n",
        "path_to_embeddings = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "embeddings = load_embeddings(path_to_embeddings, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22MDvyMSGoXE"
      },
      "source": [
        "We will now calculate the size of the largest document and create a matrix whose rows correspond to documents. Each row contains the indices of the terms appearing in the document and preserves the order of the terms in the document. That is, the first component of a row contains the index of the first term of the corresponding document, the second component contains the index of the second term etc. Documents whose length is shorter than that of the longest document are padded with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-o98AkcGo3W"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-eEr5IdGqsT"
      },
      "source": [
        "We will then use the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function of scikit-learn to split our dataset randomly into a training and a test set. Set the size of the test set to 0.1 and the random_state to 91."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41m25hqsGrJc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfYahwbZGtfM"
      },
      "source": [
        "## Defining the CNN\n",
        "For efficiency reasons, we will only implement two branches of the following architecture: \n",
        "\n",
        "<img src=\"https://resourcesftp.blob.core.windows.net/files/dssp/deep_learning/figures/cnn_illustration.png\"/>\n",
        "\n",
        "A branch is the part of the architecture that corresponds to a given filter size (e.g., the upper red part is one branch).\n",
        "\n",
        "Up until now, we have only made use of the Sequential model. The Sequential model offers limited flexibility and may not be suitable for neural networks with multiple inputs and outputs. On the other hand, the [functional API](https://keras.io/getting-started/functional-api-guide/) makes it easy to manipulate a large number of intertwined datastreams. Our CNN consists of two branches whose outputs are concatenated to produce a single vector representation for each document (i.e., the multicolor vector shown in the figure above). We will use the functional API to implement the CNN. Unlike the Sequential model, in the case of the functional API it is necessary to create and define a standalone Input layer that specifies the shape of input data. The input layer takes as input a tuple that indicates the dimensionality of the input data. When the input data is one-dimensional (as in our case), the shape must explicitly leave room for the shape of the mini-batch size. Therefore, the shape tuple is always defined with a hanging last dimension when the input is one-dimensional:\n",
        "```\n",
        "my_input = Input(shape=(dimension,))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBgx-4uiGt1V"
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyEZ0l79Gw37"
      },
      "source": [
        "The layers in the model are connected pairwise. Hence, each layer takes as input either the input data (e.g., the input layer) or the output of another layer. We will first define and [Embedding layer](https://keras.io/layers/embeddings/). The Embedding layer requires the input data to be integer encoded, so that each word is represented by a unique integer. The Embedding layer can be initialized either with random weights and learn an embedding for all of the words in the training set or with pre-trained word embeddings. In our case, it will be initialized with the 300-dimensional word embeddings that we have already loaded. The Embedding layer must specify 3 arguments: (1) `input_dim`: the size of the vocabulary, (2) `output_dim`: the size of the vector space in which the words have been embedded (i.e., 300 in our case), and (3) `input_length`: the maximum length of the input documents. In case we initialize the layer with pre-trained embeddings, we must provide another argument (`weights`) which is list that contains a matrix whose i-th row contain the embedding of term with index i. For example, below we define an Embedding layer with a vocabulary of 100, embedding dimensionality equal to 64, maximum length of the input documents equal to 50, and the embeddings are contained in the matrix embed_matrix.\n",
        "```\n",
        "embedding = Embedding(input_dim=100,\n",
        "                      output_dim=64,\n",
        "                      weights=[embed_matrix],\n",
        "                      input_length=50\n",
        "                      ) (my_input)\n",
        "```\n",
        "\n",
        "Note that we also specify where the input comes from when defining a layer. Implement the embedding layer of the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ldURZCGxa0"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3WR-0f6GzkE"
      },
      "source": [
        "We next create the two branches of the CNN. Each branch takes as input the output of the Embedding layer and applies a [one-dimensional convolution layer](https://keras.io/layers/convolutional/#conv1d) followed by a [one-dimensional max-pooling operation](https://keras.io/layers/pooling/#maxpooling1d). \n",
        "\n",
        "The one-dimensional convolution layer must specify 3 arguments: (1) `filters`: the number of filters, (2) `kernel_size`: the length of the one-dimensional convolution window, and (3) `activation`: the activation function to use. For example, below we define an one-dimensional convolution layer with 100 filters, a convolution window of size 4, and a ReLU activation function.\n",
        "```\n",
        "conv = Conv1D(filters = 100,\n",
        "              kernel_size = 4,\n",
        "              activation = 'relu',\n",
        "              )(embedding)\n",
        "```\n",
        "\n",
        "The one-dimensional max-pooling operation just takes as input the output of the convolution layer. Implement the two branches of the CNN. For each branch, implement a one-dimensional convolution layer and a one-dimensional max-pooling operation. Use 100 filters and set the size of the filters of the first branch to 3 and the size of the filters of the second branch to 4. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXkjr_t-Gz-C"
      },
      "source": [
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_quDcRxG1ys"
      },
      "source": [
        "We next introduce a layer that [concatenates](https://keras.io/layers/merge/#concatenate_1) the outputs of the two branches. This layer takes as input the two vectors that were produced from the two branches and returns a single vector, the concatenation of the two inputs. We then add dropout (rate equal to 0.5) and finally a fully-connected neural network with 2 neurons that will serve as out ouput."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw78MUgdG4TG"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout, Dense, Concatenate\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LEt_93VG6Ic"
      },
      "source": [
        "After creating all of the layers and connecting them together, we can define the model. Keras provides a [Model class](https://keras.io/models/model/) that we can use to create a model from your created layers. It requires that you only specify the input and output layers. For example, given the an input and an output, we can define the model as follows:\n",
        "```\n",
        "model = Model(input, output)\n",
        "```\n",
        "After creating the model, you can compile it. Use the Adam optimizer to minimize the categorical crossentropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmvWY42rG6mP"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sFT_dCSHEsD"
      },
      "source": [
        "We finally print the details of the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZe19PcAHFqE"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtv8rtWsHG4W"
      },
      "source": [
        "### Visualization of document embeddings before training\n",
        "We extract the output of the final embedding layer (before the softmax), which gives the encoding of the input document for some documents (`n_plot`) of the test set. We then visualize a low-dimensional map of the embeddings. We can see that before training, the documents are dispersed randomly in the space (which makes sense)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfZF3nlOHHRq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# in test mode, we should set the 'learning_phase' flag to 0 (we don't want to use dropout)\n",
        "get_doc_embedding = K.function(model.inputs,\n",
        "                               [model.layers[6].output])\n",
        "\n",
        "n_plot = 1000\n",
        "print('plotting embeddings of first',n_plot,'documents')\n",
        "\n",
        "doc_emb = get_doc_embedding([np.array(X_test[:n_plot]),0])[0]\n",
        "\n",
        "my_pca = PCA(n_components=10)\n",
        "my_tsne = TSNE(n_components=2,perplexity=10) #https://lvdmaaten.github.io/tsne/\n",
        "doc_emb_pca = my_pca.fit_transform(doc_emb) \n",
        "doc_emb_tsne = my_tsne.fit_transform(doc_emb_pca)\n",
        "\n",
        "labels_plt = y_test[:n_plot,0].astype(np.int32)\n",
        "my_colors = ['blue','red']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for label in list(set(labels_plt)):\n",
        "  idxs = [idx for idx,elt in enumerate(labels_plt) if elt==label]\n",
        "  ax.scatter(doc_emb_tsne[idxs,0], \n",
        "              doc_emb_tsne[idxs,1], \n",
        "              c = my_colors[label],\n",
        "              label=str(label),\n",
        "              alpha=0.7,\n",
        "              s=10)\n",
        "\n",
        "ax.legend(scatterpoints=1)\n",
        "fig.suptitle('t-SNE visualization of CNN-based doc embeddings \\n (first 1000 docs from test set)',fontsize=10)\n",
        "fig.set_size_inches(6,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9-9dPeHHIv3"
      },
      "source": [
        "### Train the CNN\n",
        "We train the model on CPU. Set the batch size to 64 and the number of epochs to 10. Note you can get a significant speedup by using a GPU. We also add two callbacks:\n",
        "* the first one ensures that training stops after 2 epochs without improvement in test set accuracy (early stopping strategy)\n",
        "* the second one (checkpointer) saves the model to disk for every epoch for which there is improvement. Therefore, at the end of training, the model saved on disk will be the one corresponding to the best epoch and we can reload it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvgkLO3kHJ6q"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', # go through epochs as long as accuracy on validation set increases\n",
        "                               patience=2,\n",
        "                               mode='max')\n",
        "\n",
        "# make sure that the model corresponding to the best epoch is saved\n",
        "checkpointer = ModelCheckpoint(filepath='cnn_text_categorization.hdf5',\n",
        "                               monitor='val_accuracy',\n",
        "                               save_best_only=True,\n",
        "                               verbose=0)\n",
        "\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWv5S_SeHL2P"
      },
      "source": [
        "We want to make sure we load the model corresponding to the best epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwuvnQ_eHMeB"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('cnn_text_categorization.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snjvRN0aHOeA"
      },
      "source": [
        "### Visualization of document embeddings after training\n",
        "We can see that after only a few epochs, our model has already learned meaningful internal representations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIFlmuvjHNMl"
      },
      "source": [
        "print('plotting embeddings of first',n_plot,'documents')\n",
        "\n",
        "doc_emb = get_doc_embedding([np.array(X_test[:n_plot]),0])[0]\n",
        "\n",
        "my_pca = PCA(n_components=10)\n",
        "my_tsne = TSNE(n_components=2,perplexity=10)\n",
        "doc_emb_pca = my_pca.fit_transform(doc_emb) \n",
        "doc_emb_tsne = my_tsne.fit_transform(doc_emb_pca)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for label in list(set(labels_plt)):\n",
        "  idxs = [idx for idx,elt in enumerate(labels_plt) if elt==label]\n",
        "  ax.scatter(doc_emb_tsne[idxs,0], \n",
        "              doc_emb_tsne[idxs,1], \n",
        "              c = my_colors[label],\n",
        "              label=str(label),\n",
        "              alpha=0.7,\n",
        "              s=10)\n",
        "\n",
        "ax.legend(scatterpoints=1)\n",
        "fig.suptitle('t-SNE visualization of CNN-based doc embeddings \\n (first 1000 docs from test set)',fontsize=10)\n",
        "fig.set_size_inches(6,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xr3-vyoHRIv"
      },
      "source": [
        "### Predictive text regions\n",
        "Here we follow the approach of [Effective Use of Word Order for Text Categorization with Convolutional Neural Networks (Johnson and Zhang, NAACL'15)](https://www.aclweb.org/anthology/N15-1011/) (see Tables 5 and 6).\n",
        "\n",
        "The feature maps that we find at the output of the convolutional layer provide region embeddings (in an `nb_filters`-dimensional space). For a given branch associated with `filter_size`, there are `max_size-filter_size+1` regions of size `filter_size` for an input of size `max_size`. For a given document, we want to identify the `n_show` regions of each branch that are associated with the highest weights in the corresponding feature maps. \n",
        "\n",
        "We can see that to classify the test set documents (which the model has never seen), the CNN uses regions of the input documents that make sense to us as humans. It picks up the compliments (`\"glaring and unforgettable\"`, `\"a good yarn spinner\"`) and critics (`\"not merely unwatchable\"`, `\"but feeling pandered\"`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK5PZNuMHR0l"
      },
      "source": [
        "def extract_regions(tokens, filter_size):\n",
        "  regions = []\n",
        "  regions.append(' '.join(tokens[:filter_size]))\n",
        "  for i in range(filter_size, len(tokens)):\n",
        "    regions.append(' '.join(tokens[(i-filter_size+1):(i+1)]))\n",
        "  return regions\n",
        "\n",
        "index_to_word = dict()\n",
        "for word in vocab:\n",
        "  index_to_word[vocab[word]] = word\n",
        "\n",
        "filter_size_a = 3\n",
        "filter_size_b = 4\n",
        "\n",
        "get_region_embedding_a = K.function(model.inputs,\n",
        "                                    [model.layers[2].output])\n",
        "\n",
        "get_region_embedding_b = K.function(model.inputs,\n",
        "                                    [model.layers[3].output])\n",
        "\n",
        "get_softmax = K.function(model.inputs,\n",
        "                         [model.layers[8].output])\n",
        "\n",
        "n_doc_per_label = 2\n",
        "idx_pos = [idx for idx in range(y_test.shape[0]) if y_test[idx,0]==1]\n",
        "idx_neg = [idx for idx in range(y_test.shape[0]) if y_test[idx,0]==0]\n",
        "my_idxs = idx_pos[:n_doc_per_label] + idx_neg[:n_doc_per_label]\n",
        "\n",
        "X_test_my_idxs = np.array([X_test[elt] for elt in my_idxs])\n",
        "y_test_my_idxs = [y_test[elt] for elt in my_idxs]\n",
        "\n",
        "reg_emb_a = get_region_embedding_a([X_test_my_idxs,0])[0]\n",
        "reg_emb_b = get_region_embedding_b([X_test_my_idxs,0])[0]\n",
        "\n",
        "# predictions are probabilities of belonging to class 1\n",
        "predictions = get_softmax([X_test_my_idxs,0])[0] \n",
        "# note: you can also use directly: predictions = model.predict(x_test[:100]).tolist()\n",
        "\n",
        "n_show = 3 # number of most predictive regions we want to display\n",
        "\n",
        "for idx,doc in enumerate(X_test_my_idxs):\n",
        "  tokens = [index_to_word[elt] for elt in doc if elt!=0] # the 0 index is for padding\n",
        "  \n",
        "  # extract regions (sliding window over text)\n",
        "  regions_a = extract_regions(tokens, filter_size_a)\n",
        "  regions_b = extract_regions(tokens, filter_size_b)\n",
        "  \n",
        "  print('\\n *********')\n",
        "  print('===== text: =====')\n",
        "  print(' '.join(tokens))\n",
        "  print('===== label:',y_test_my_idxs[idx],'=====')\n",
        "  print('===== prediction:',predictions[idx],'=====')\n",
        "  norms_a = np.linalg.norm(reg_emb_a[idx,:,:],axis=1)\n",
        "  norms_b = np.linalg.norm(reg_emb_b[idx,:,:],axis=1)\n",
        "  print('===== most predictive regions of size',filter_size_a,': =====')\n",
        "  print([elt for idxx,elt in enumerate(regions_a) if idxx in np.argsort(norms_a)[-n_show:]]) # 'np.argsort' sorts by increasing order\n",
        "  print('===== most predictive regions of size',filter_size_b,': =====')\n",
        "  print([elt for idxx,elt in enumerate(regions_b) if idxx in np.argsort(norms_b)[-n_show:]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49vw2-9XHT5c"
      },
      "source": [
        "### Saliency maps\n",
        "Here we follow one of the approaches proposed in [Visualizing and Understanding Neural Models in NLP (Li et al., NAACL'16)](http://www.aclweb.org/anthology/N16-1082).\n",
        "\n",
        "The idea is to rank the elements of the input document based on their influence on the prediction. An approximation can be given by the magnitudes of the first-order partial derivatives of the output of the model with respect to each word in the input document. The interpretation is that we identify which words in the document need to be *changed the least to change the class score the most*. The derivatives can be obtained by performing a single back-propagation pass. Note that here, we backpropagate the class score and not the loss (like we do during training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOu9MbOaHUdP"
      },
      "source": [
        "saliency_input = model.layers[1].output # before split into branches\n",
        "saliency_output = model.layers[8].output[:,0] # class score\n",
        "gradients = model.optimizer.get_gradients(saliency_output,saliency_input)\n",
        "compute_gradients = K.function(inputs=model.inputs,outputs=gradients)\n",
        "\n",
        "for idx,doc in enumerate(X_test_my_idxs):\n",
        "  matrix = compute_gradients([np.array([doc]),0])[0][0,:,:]\n",
        "  tokens = [index_to_word[elt] for elt in doc if elt!=0]\n",
        "  to_plot = np.absolute(matrix[:len(tokens),:])\n",
        "  fig, ax = plt.subplots()\n",
        "  heatmap = ax.imshow(to_plot, cmap=plt.cm.Blues, interpolation='nearest',aspect='auto')\n",
        "  ax.set_yticks(np.arange(len(tokens)))\n",
        "  ax.set_yticklabels(tokens)\n",
        "  ax.tick_params(axis='y', which='major', labelsize=32*10/len(tokens))\n",
        "  fig.colorbar(heatmap)\n",
        "  fig.set_size_inches(14,9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqgluDGw0bXd"
      },
      "source": [
        "## Distances in word embeddings\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/890/1*nTWAm46JMYWXpHVsS9MA5w.png\"/>\n",
        "\n",
        "Compute the cosine similarity with mean embeddings and word mover's distance with gensim. First create a function for getting the mean embedding given a sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmry1Jke0bXd"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "from gensim.similarities import WmdSimilarity\n",
        "\n",
        "## your code "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}